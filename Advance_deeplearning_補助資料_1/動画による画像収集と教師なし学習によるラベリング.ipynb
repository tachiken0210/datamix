{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動画による画像収集と教師なし学習によるラベリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "動画を用いて画像データを効率的に集める方法と効率的にラベリングする方法を学びます。  \n",
    "\n",
    "\n",
    "以下、流れになります。  \n",
    "①画像データを効率的に集めるため、動画からフレーム毎に顔画像をトリミングし保存（openCVという画像認識ライブラリを使用します。）  \n",
    "②ラベリングを効率的に行うため、教師なし学習により分類しラベリング  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用するライブラリをインストールします。（他に必要なライブラリがあれば個別にインストールしてください）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open cvのインストール\n",
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scipyのインストール\n",
    "! pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.misc\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずopenCVで使用するカスケードファイルとそのファイルの場所を指定します。今回はhaarcascade_frontalface_alt2.xmlというカスケードファイルを使います。このファイルを使用することで顔と顔でないものを分類することが可能となります。  \n",
    "以下のコマンドを実行してください。ご自身のパソコン内のカスケードファイルの場所が表示されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mdfind haarcascade_frontalface_alt2.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上で表示された場所（/Users/yuza/anaconda3/lib/python3.6/site-packages/cv2/data/haarcascade_frontalface_alt2.xmlに似ている行）をコピーし、下の部分を置き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cascadeファイルのパス指定\n",
    "cascade_path = \"/Users/yuza/anaconda3/lib/python3.6/site-packages/cv2/data/haarcascade_frontalface_alt2.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動画から顔画像のトリミング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は簡単な例としてデータミックス のスクール紹介動画の１部分を使用します。  \n",
    "openCVを使用すると顔を自動的に認識でき顔部分の座標が取得できますので、それを元にトリミングし保存します。また元画像はRGBですが、色による影響を少なくするために白黒に変換します。 \n",
    "トリムした画像はtrim_imageという名前のフォルダに保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#画像から顔認識\n",
    "def detect_face(image,cascade):\n",
    "    facerect = cascade.detectMultiScale(\n",
    "        image,\n",
    "        scaleFactor=1.5,\n",
    "        minNeighbors=3,\n",
    "        minSize=(40, 40))\n",
    "    return facerect\n",
    "  \n",
    "#顔画像のみトリムし保存\n",
    "def trim_face_and_save(facerect,image,file_path,file_name,count):\n",
    "    if 0 != len(facerect):\n",
    "        for i,r in enumerate(facerect):\n",
    "            x = r[0]\n",
    "            y=r[1]\n",
    "            width = r[2]\n",
    "            height = r[3]\n",
    "            cut_image = image[y:y+height,x:x+width]\n",
    "            savepath = \"{}/{}_{}_{}.png\".format(file_path, file_name,count,i)\n",
    "            cv2.imwrite(savepath,cut_image)\n",
    "\n",
    "\n",
    "def addrectangle_to_pic(image,facerect):\n",
    "    color = (255, 255, 255)\n",
    "    for rect in facerect:\n",
    "        cv2.rectangle(image, tuple(rect[0:2]),tuple(rect[0:2]+rect[2:4]), color, thickness=2)\n",
    "        \n",
    "#動画から顔のみトリムし画像保存\n",
    "def trim_from_video(cascade,file_path):\n",
    "    count = 0\n",
    "    cap = cv2.VideoCapture(ORG_FILE_NAME)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:   \n",
    "            facerect = detect_face(frame,cascade)\n",
    "            if 0 != len(facerect):\n",
    "                count += 1\n",
    "                print(\"face deteted! total_num = {0}\".format(count))\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                trim_face_and_save(facerect,gray,file_path,\"trim\",count)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "            \n",
    "#対象の動画ファイルパス指定\n",
    "ORG_FILE_NAME = \"./movie/sample_datamix.mp4\"\n",
    "\n",
    "#対象の動画ファイルパス指定\n",
    "cascade = cv2.CascadeClassifier(cascade_path)\n",
    "\n",
    "#保存フォルダ名指定\n",
    "folder_name = \"trim_image\"\n",
    "file_path = \"./{}\".format(folder_name)\n",
    "\n",
    "#フォルダ作成\n",
    "try:\n",
    "    os.mkdir(\"{}\".format(folder_name))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "trim_from_video(cascade,file_path)\n",
    "print(\"trim finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行を終えるとtrim_imageというフォルダの下に顔のみトリムされた画像が配置されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画像の配列化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トリムした画像を配列として読み込みます。データのサイズを揃えるため、一定の解像度に落とします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#白黒反転\n",
    "def bitwise(data):\n",
    "    data = cv2.bitwise_not(data)\n",
    "    return data\n",
    "\n",
    "count = 0\n",
    "list_img_vector = []\n",
    "\n",
    "#トリムした顔画像pngファイルを読み込む\n",
    "list_images = glob.glob('{}/*.png'.format(file_path))\n",
    "base_list_images = [os.path.basename(r) for r in glob.glob('{}/*.png'.format(file_path))]\n",
    "\n",
    "\n",
    "#読み込んだ画像を配列化する\n",
    "for f in list_images:\n",
    "    img = cv2.imread('{}'.format(f),0)\n",
    "    img = bitwise(img)\n",
    "    vector = np.asarray(img/255)\n",
    "    #画像を100×100に解像度を落とし、揃えます。\n",
    "    vector = scipy.misc.imresize(vector, (100, 100))\n",
    "    list_img_vector.append(vector)\n",
    "    array_img_vector = np.array(list_img_vector)\n",
    "\n",
    "print(array_img_vector.shape)\n",
    "\n",
    "#     plt.imshow(vector.reshape(28,28),cmap=\"gray_r\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ラベリングのための教師なし学習による分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "教師あり学習を行うためには学習データにラベリングが必要ですが、その作業を効率化するため、今回は教師なし学習を用います。（他にラベリングを効率化する手段としてアクティブラーニングや半教師あり学習があります。）\n",
    "まずTsneによる次元削減を行い、その後Kmeansによるクラスタリングを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tsneによる次元削減\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#画像をフラットな配列に変換します\n",
    "reshaped_array = np.reshape(array_img_vector,[-1,100*100])\n",
    "reduced = TSNE(n_components=2, random_state=0).fit_transform(reshaped_array)\n",
    "\n",
    "plt.scatter(reduced[:, 0], reduced[:, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大きく分けて２つに分類できそうです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#念のためエルボー法でSSEを確認\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "SSE = []\n",
    "for i in range(1,20):\n",
    "    km = KMeans(n_clusters=i,random_state=0)\n",
    "    pred = km.fit_predict(reduced)\n",
    "    SSE.append(km.inertia_)\n",
    "    \n",
    "plt.plot(range(1,20),SSE,marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クラスタ数を２としてKmeansにより分類してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#クラスタ数を決定しクラスタリングを行う\n",
    "km = KMeans(n_clusters=2,random_state=0)\n",
    "pred = km.fit_predict(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#どのように分類されたか可視化\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "target_names = set(pred)\n",
    "colors = str('rgbcmykw')\n",
    "target_ids = range(len(target_names))\n",
    "plt.figure()\n",
    "for i, c, label in zip(target_ids, colors, target_names):\n",
    "    plt.scatter(reduced[pred == i, 0], reduced[pred == i, 1],\n",
    "                    c=c, label=label)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得られたラベルの情報を元に画像のファイルをフォルダに振り分けます。\n",
    "今回は今までの解像度を落とした画像を保存するのではなく、確認を容易にするため元の解像度の画像を保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分類結果を元に画像をそれぞれフォルダに振り分ける\n",
    "import shutil\n",
    "\n",
    "array_file_pred = np.vstack([np.array(pred), np.array(base_list_images)]).T\n",
    "dic_file_pred = {}\n",
    "\n",
    "for i in set(pred):\n",
    "    try:\n",
    "        os.mkdir(\"./trim_image/cluster_{}\".format(str(i)))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for f in array_file_pred[:,1][array_file_pred[:,0]==str(i)]:\n",
    "        shutil.move(\"./trim_image/{}\".format(f), \"./trim_image/cluster_{}\".format(str(i)))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "振り分けられたフォルダの中身を確認してみると、（完全に正確な訳ではないですが）ほぼ正確に分類されていることがわかります。  \n",
    "もし誤分類されているデータを見つけたら消去してしまいましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ラベリングのための教師なし学習による分類\n",
    "次に上で行なった振り分けを元にデータセットを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = \"{}/trim_image\".format(os.getcwd())\n",
    "folders = os.listdir(img_folder)\n",
    "\n",
    "X_train =[]\n",
    "y_train = []\n",
    "\n",
    "for folder in folders:\n",
    "    if \"cluster_\" in folder:\n",
    "        list_images = glob.glob('{}/{}/*.png'.format(img_folder,folder))\n",
    "        for f in list_images:\n",
    "            img = cv2.imread('{}'.format(f),0)\n",
    "            img = bitwise(img)\n",
    "            vector = np.asarray(img)\n",
    "            #以下で解像度を落としています\n",
    "            vector = scipy.misc.imresize(vector, (100, 100))\n",
    "            X_train.append(vector/255)\n",
    "            \n",
    "            cluster_index = folder.replace(\"cluster_\",\"\") \n",
    "            y_train.append(cluster_index) \n",
    "\n",
    "#画像を100×100に解像度を落とし、揃えます。\n",
    "X_train = np.concatenate(X_train).reshape(-1,100,100)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで教師あり学習に用いるデータセットの作成ができました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
